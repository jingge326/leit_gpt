{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pathlib\n",
                "from datetime import timedelta\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "p_project = str(pathlib.Path(os.getcwd()).parents[1])\n",
                "random_state = 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "path_temp = p_project + '/data/mimic4'\n",
                "processed_balanced = path_temp + '/processed_balanced_r4'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "mimic4_df = pd.read_csv(processed_balanced + '/mimic4_full_dataset.csv', index_col='ID')\n",
                "\n",
                "all_ids = pd.DataFrame(mimic4_df.index.unique(), columns=['ID'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Time</th>\n",
                            "      <th>Value_label_0</th>\n",
                            "      <th>Mask_label_0</th>\n",
                            "      <th>Value_label_2</th>\n",
                            "      <th>Mask_label_2</th>\n",
                            "      <th>Value_label_5</th>\n",
                            "      <th>Mask_label_5</th>\n",
                            "      <th>Value_label_6</th>\n",
                            "      <th>Mask_label_6</th>\n",
                            "      <th>Value_label_8</th>\n",
                            "      <th>...</th>\n",
                            "      <th>Value_label_82</th>\n",
                            "      <th>Mask_label_82</th>\n",
                            "      <th>Value_label_4</th>\n",
                            "      <th>Mask_label_4</th>\n",
                            "      <th>Value_label_94</th>\n",
                            "      <th>Mask_label_94</th>\n",
                            "      <th>Value_label_95</th>\n",
                            "      <th>Mask_label_95</th>\n",
                            "      <th>Value_label_92</th>\n",
                            "      <th>Mask_label_92</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>ID</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>20000147</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>20000147</th>\n",
                            "      <td>578</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>20000147</th>\n",
                            "      <td>599</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>20000147</th>\n",
                            "      <td>693</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>20000147</th>\n",
                            "      <td>720</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>5 rows Ã— 193 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "          Time  Value_label_0  Mask_label_0  Value_label_2  Mask_label_2  \\\n",
                            "ID                                                                         \n",
                            "20000147     0            0.0             0            0.0             0   \n",
                            "20000147   578            0.0             0            0.0             0   \n",
                            "20000147   599            0.0             0            0.0             0   \n",
                            "20000147   693            0.0             0            0.0             0   \n",
                            "20000147   720            0.0             0            0.0             0   \n",
                            "\n",
                            "          Value_label_5  Mask_label_5  Value_label_6  Mask_label_6  \\\n",
                            "ID                                                                   \n",
                            "20000147            0.0             0            0.0             0   \n",
                            "20000147            0.0             0            0.0             0   \n",
                            "20000147            0.0             0            0.0             0   \n",
                            "20000147            0.0             0            0.0             0   \n",
                            "20000147            0.0             0            0.0             0   \n",
                            "\n",
                            "          Value_label_8  ...  Value_label_82  Mask_label_82  Value_label_4  \\\n",
                            "ID                       ...                                                 \n",
                            "20000147            0.0  ...             0.0              0            0.0   \n",
                            "20000147            0.0  ...             0.0              0            0.0   \n",
                            "20000147            0.0  ...             0.0              0            0.0   \n",
                            "20000147            0.0  ...             0.0              0            0.0   \n",
                            "20000147            0.0  ...             0.0              0            0.0   \n",
                            "\n",
                            "          Mask_label_4  Value_label_94  Mask_label_94  Value_label_95  \\\n",
                            "ID                                                                      \n",
                            "20000147             0             0.0              0             0.0   \n",
                            "20000147             0             0.0              0             0.0   \n",
                            "20000147             0             0.0              0             0.0   \n",
                            "20000147             0             0.0              0             0.0   \n",
                            "20000147             0             0.0              0             0.0   \n",
                            "\n",
                            "          Mask_label_95  Value_label_92  Mask_label_92  \n",
                            "ID                                                      \n",
                            "20000147              0             0.0              0  \n",
                            "20000147              0             0.0              0  \n",
                            "20000147              0             0.0              0  \n",
                            "20000147              0             0.0              0  \n",
                            "20000147              0             0.0              0  \n",
                            "\n",
                            "[5 rows x 193 columns]"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "mimic4_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pretraining dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "pretraining_ids = all_ids.sample(n=40000, random_state=random_state)\n",
                "\n",
                "mimic4_pretraining_df = mimic4_df.loc[pretraining_ids['ID'].unique()]\n",
                "\n",
                "mimic4_pretraining_df.to_csv(processed_balanced + '/data_mimic4_pretraining_40k_r{}.csv'.format(str(random_state)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "tvt_ids = all_ids.drop(pretraining_ids.index)\n",
                "\n",
                "tvt_ids.to_csv(processed_balanced + '/tvt_ids.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mortality dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "tvt_ids = pd.read_csv(processed_balanced + '/tvt_ids.csv', index_col=0)\n",
                "\n",
                "adm = pd.read_csv(path_data + '/core/admissions.csv.gz', compression='gzip')\n",
                "\n",
                "tvt_mortality_labels = adm[adm['hadm_id'].isin(tvt_ids['ID'])][['hadm_id', 'hospital_expire_flag']].rename(columns={'hadm_id': 'ID', 'hospital_expire_flag': 'labels'})\n",
                "\n",
                "tvt_mortality_labels.reset_index(drop=True).to_csv(processed_balanced + '/mortality_labels.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "mortality_test_ids = tvt_ids.sample(n=1000, random_state=random_state)\n",
                "\n",
                "mimic4_mortality_test_df = mimic4_df.loc[mortality_test_ids['ID']]\n",
                "\n",
                "mimic4_mortality_test_df.to_csv(processed_balanced + '/data_mimic4_mortality_test.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "mortality_tv_ids = tvt_ids.drop(mortality_test_ids.index)\n",
                "\n",
                "death_ids = adm[adm['hospital_expire_flag']==1]['hadm_id']\n",
                "\n",
                "survived_ids = adm[adm['hospital_expire_flag']!=1]['hadm_id']\n",
                "\n",
                "tv_ids_death = mortality_tv_ids[mortality_tv_ids['ID'].isin(death_ids)]\n",
                "\n",
                "tv_ids_survived = mortality_tv_ids[mortality_tv_ids['ID'].isin(survived_ids)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_balanced_few_shot_datasets_mortality(num, ids_positive, ids_negative, path_save):\n",
                "    few_shot_pos = ids_positive.sample(n=num//2, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_neg = ids_negative.sample(n=num//2, random_state=random_state)['ID'].to_list()\n",
                "    few_shot = few_shot_pos + few_shot_neg\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    df_train = pd.DataFrame(few_shot_train, columns =['ID'])\n",
                "    df_valid = pd.DataFrame(few_shot_valid, columns =['ID'])\n",
                "    df_train.to_csv(path_save + '/m4_mortality_train_{}.csv'.format(num))\n",
                "    df_valid.to_csv(path_save + '/m4_mortality_valid_{}.csv'.format(num))\n",
                "    few_shot_train_df=mimic4_df.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=mimic4_df.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save + '/m4_mortality_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save + '/m4_mortality_{}_valid.csv'.format(num), index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "generate_balanced_few_shot_datasets_mortality(num=100, ids_positive=tv_ids_death, ids_negative=tv_ids_survived, path_save=processed_balanced)\n",
                "generate_balanced_few_shot_datasets_mortality(num=250, ids_positive=tv_ids_death, ids_negative=tv_ids_survived, path_save=processed_balanced)\n",
                "generate_balanced_few_shot_datasets_mortality(num=500, ids_positive=tv_ids_death, ids_negative=tv_ids_survived, path_save=processed_balanced)\n",
                "generate_balanced_few_shot_datasets_mortality(num=1000, ids_positive=tv_ids_death, ids_negative=tv_ids_survived, path_save=processed_balanced)\n",
                "generate_balanced_few_shot_datasets_mortality(num=2000, ids_positive=tv_ids_death, ids_negative=tv_ids_survived, path_save=processed_balanced)\n",
                "generate_balanced_few_shot_datasets_mortality(num=3000, ids_positive=tv_ids_death, ids_negative=tv_ids_survived, path_save=processed_balanced)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Length dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_22492/3091813934.py:7: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  hosp_stay['admittime']=pd.to_datetime(hosp_stay['admittime'], format='%Y-%m-%d %H:%M:%S')\n",
                        "/tmp/ipykernel_22492/3091813934.py:8: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  hosp_stay['dischtime']=pd.to_datetime(hosp_stay['dischtime'], format='%Y-%m-%d %H:%M:%S')\n",
                        "/tmp/ipykernel_22492/3091813934.py:10: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
                        "Try using .loc[row_indexer,col_indexer] = value instead\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  hosp_stay['stayed_hour'] = ((hosp_stay['dischtime'] - hosp_stay['admittime']) / pd.Timedelta(hours=1)).apply(np.ceil)\n"
                    ]
                }
            ],
            "source": [
                "tvt_ids = pd.read_csv(processed_balanced + '/tvt_ids.csv')[['ID']]\n",
                "\n",
                "adm = pd.read_csv(path_data + '/core/admissions.csv.gz', compression='gzip')\n",
                "\n",
                "hosp_stay = adm[['hadm_id', 'admittime', 'dischtime', 'hospital_expire_flag']]\n",
                "\n",
                "hosp_stay['admittime']=pd.to_datetime(hosp_stay['admittime'], format='%Y-%m-%d %H:%M:%S')\n",
                "hosp_stay['dischtime']=pd.to_datetime(hosp_stay['dischtime'], format='%Y-%m-%d %H:%M:%S')\n",
                "\n",
                "hosp_stay['stayed_hour'] = ((hosp_stay['dischtime'] - hosp_stay['admittime']) / pd.Timedelta(hours=1)).apply(np.ceil)\n",
                "\n",
                "hosp_stay_survived = hosp_stay[hosp_stay['hospital_expire_flag']!=1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "tvt_los_labels = hosp_stay_survived[hosp_stay_survived['hadm_id'].isin(tvt_ids['ID'])].rename(columns={'hadm_id': 'ID'})[['ID', 'stayed_hour']]\n",
                "\n",
                "tvt_los_labels.reset_index(drop=True).to_csv(processed_balanced + '/length_labels.csv')\n",
                "\n",
                "tvt_ids_survived = tvt_los_labels[['ID']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "length_test_ids = tvt_ids_survived.sample(n=1000, random_state=random_state)\n",
                "\n",
                "mimic4_length_test_df = mimic4_df.loc[length_test_ids['ID']]\n",
                "\n",
                "mimic4_length_test_df.to_csv(processed_balanced + '/data_mimic4_length_test.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "length_tv_ids = tvt_ids_survived.drop(length_test_ids.index)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_few_shot_datasets_length(num, ids, path_save):\n",
                "    few_shot = ids.sample(n=num, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    df_train = pd.DataFrame(few_shot_train, columns =['ID'])\n",
                "    df_valid = pd.DataFrame(few_shot_valid, columns =['ID'])\n",
                "    df_train.to_csv(path_save + '/m4_length_train_{}.csv'.format(num))\n",
                "    df_valid.to_csv(path_save + '/m4_length_valid_{}.csv'.format(num))\n",
                "    few_shot_train_df=mimic4_df.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=mimic4_df.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save + '/m4_length_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save + '/m4_length_{}_valid.csv'.format(num), index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "generate_few_shot_datasets_length(num=100, ids=length_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_length(num=250, ids=length_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_length(num=500, ids=length_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_length(num=1000, ids=length_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_length(num=2000, ids=length_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_length(num=3000, ids=length_tv_ids, path_save=processed_balanced)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Next3 dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_2174347/3779396662.py:5: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                        "  inputs_df = pd.read_csv(path_temp + '/processed/tables/inputs_processed.csv')[\n",
                        "/tmp/ipykernel_2174347/3779396662.py:9: DtypeWarning: Columns (8,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                        "  presc_df = pd.read_csv(path_temp + '/processed/tables/prescriptions_processed.csv')[\n",
                        "/tmp/ipykernel_2174347/3779396662.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
                        "  merged_df1 = (inputs_df.append(lab_df)).reset_index()\n",
                        "/tmp/ipykernel_2174347/3779396662.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
                        "  merged_df2 = (merged_df1.append(outputs_df)).reset_index()\n",
                        "/tmp/ipykernel_2174347/3779396662.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
                        "  merged_df = (merged_df2.append(presc_df)).reset_index()\n"
                    ]
                }
            ],
            "source": [
                "tvt_ids = pd.read_csv(processed_balanced + '/tvt_ids.csv')[['ID']]\n",
                "\n",
                "lab_df = pd.read_csv(path_temp + '/processed/tables/lab_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'valuenum', 'label']]\n",
                "inputs_df = pd.read_csv(path_temp + '/processed/tables/inputs_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'amount', 'label']]\n",
                "outputs_df = pd.read_csv(path_temp + '/processed/tables/outputs_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'value', 'label']]\n",
                "presc_df = pd.read_csv(path_temp + '/processed/tables/prescriptions_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'dose_val_rx', 'drug']]\n",
                "\n",
                "lab_df = lab_df[lab_df['hadm_id'].isin(tvt_ids['ID'])]\n",
                "inputs_df = inputs_df[inputs_df['hadm_id'].isin(tvt_ids['ID'])]\n",
                "outputs_df = outputs_df[outputs_df['hadm_id'].isin(tvt_ids['ID'])]\n",
                "presc_df = presc_df[presc_df['hadm_id'].isin(tvt_ids['ID'])]\n",
                "\n",
                "# Change the name of amount. Valuenum for every table\n",
                "inputs_df['valuenum'] = inputs_df['amount']\n",
                "inputs_df = inputs_df.drop(columns=['amount']).copy()\n",
                "\n",
                "outputs_df['valuenum'] = outputs_df['value']\n",
                "outputs_df = outputs_df.drop(columns=['value']).copy()\n",
                "\n",
                "presc_df['valuenum'] = presc_df['dose_val_rx']\n",
                "presc_df = presc_df.drop(columns=['dose_val_rx']).copy()\n",
                "presc_df['label'] = presc_df['drug']\n",
                "presc_df = presc_df.drop(columns=['drug']).copy()\n",
                "\n",
                "# Tag to distinguish between lab and inputs events\n",
                "inputs_df['Origin'] = 'Inputs'\n",
                "lab_df['Origin'] = 'Lab'\n",
                "outputs_df['Origin'] = 'Outputs'\n",
                "presc_df['Origin'] = 'Prescriptions'\n",
                "\n",
                "# merge both dfs.\n",
                "merged_df1 = (inputs_df.append(lab_df)).reset_index()\n",
                "merged_df2 = (merged_df1.append(outputs_df)).reset_index()\n",
                "merged_df2.drop(columns='level_0', inplace=True)\n",
                "merged_df = (merged_df2.append(presc_df)).reset_index()\n",
                "\n",
                "# Check that all labels have different names.\n",
                "assert(merged_df['label'].nunique() == (inputs_df['label'].nunique(\n",
                ")+lab_df['label'].nunique()+outputs_df['label'].nunique()+presc_df['label'].nunique()))\n",
                "\n",
                "# set the timestamp as the time delta between the first chart time for each admission\n",
                "merged_df['charttime'] = pd.to_datetime(\n",
                "    merged_df['charttime'], format='%Y-%m-%d %H:%M:%S')\n",
                "ref_time = merged_df.groupby('hadm_id')['charttime'].min()\n",
                "merged_df_1 = pd.merge(ref_time.to_frame(name='ref_time'),\n",
                "                       merged_df, left_index=True, right_on='hadm_id')\n",
                "merged_df_1['time_stamp'] = merged_df_1['charttime']-merged_df_1['ref_time']\n",
                "assert(len(merged_df_1.loc[merged_df_1['time_stamp']\n",
                "       < timedelta(hours=0)].index) == 0)\n",
                "\n",
                "# Create a label code (int) for the labels.\n",
                "label_dict = dict(zip(list(merged_df_1['label'].unique()), range(\n",
                "    len(list(merged_df_1['label'].unique())))))\n",
                "merged_df_1['label_code'] = merged_df_1['label'].map(label_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2174347/2587787799.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df_gb = complete_df.groupby(['ID', 'Time'], as_index=False).max()\n",
                        "/tmp/ipykernel_2174347/2587787799.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df_gb = complete_df.groupby(['ID', 'Time'], as_index=False).max()\n"
                    ]
                }
            ],
            "source": [
                "merged_df_short = merged_df_1[['hadm_id', 'valuenum', 'time_stamp', 'label_code', 'Origin']].rename(\n",
                "    columns={'hadm_id': 'ID', 'time_stamp': 'Time'})\n",
                "\n",
                "# Make sure that the selected admissions have observations after 24 hours\n",
                "ids_selected = merged_df_short[merged_df_short['Time'] > timedelta(hours=24)]['ID'].unique()\n",
                "# select only values within first 48 hours\n",
                "merged_df_short = merged_df_short[merged_df_short['ID'].isin(ids_selected)].loc[(merged_df_short['Time'] < timedelta(hours=48))]\n",
                "\n",
                "merged_df_short['Time'] = merged_df_short['Time'].dt.total_seconds().div(60).astype(int)\n",
                "assert(len(merged_df_short.loc[merged_df_short['Time'] > 2880].index) == 0)\n",
                "\n",
                "# drop columns that are not needed for final dataset\n",
                "merged_df_short.drop(['Origin'], axis=1, inplace=True)\n",
                "complete_df = merged_df_short\n",
                "\n",
                "# create value- and mask- columns and fill with data\n",
                "labels = complete_df['label_code'].unique()\n",
                "value_columns = []\n",
                "mask_columns = []\n",
                "for num in labels:\n",
                "    name = 'Value_label_' + str(num)\n",
                "    name2 = 'Mask_label_' + str(num)\n",
                "    value_columns.append(name)\n",
                "    mask_columns.append(name2)\n",
                "    complete_df[name] = 0.0\n",
                "    complete_df[name2] = 0\n",
                "    \n",
                "complete_df.dropna(inplace=True)\n",
                "\n",
                "for index, row in complete_df.iterrows():\n",
                "    name = 'Value_label_' + str(row['label_code'].astype(int))\n",
                "    name2 = 'Mask_label_' + str(row['label_code'].astype(int))\n",
                "    complete_df.at[index, name] = row['valuenum']\n",
                "    complete_df.at[index, name2] = 1\n",
                "\n",
                "# drop all unneccesary columns and do sanity check\n",
                "complete_df.drop(['valuenum', 'label_code'], axis=1, inplace=True)\n",
                "\n",
                "# If there are multiple values for the same time stamp, take the maximum\n",
                "complete_df_gb = complete_df.groupby(['ID', 'Time'], as_index=False).max()\n",
                "\n",
                "for x in mask_columns:\n",
                "    assert(len(complete_df_gb.loc[complete_df_gb[x] > 1]) == 0)\n",
                "complete_df_gb['ID'] = complete_df_gb['ID'].astype(int)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "complete_df_gb.set_index(['ID'], inplace=True)\n",
                "tvt_ids = tvt_ids[tvt_ids['ID'].isin(complete_df_gb.index)]\n",
                "next3_test_ids = tvt_ids.sample(n=1000, random_state=random_state)\n",
                "mimic4_next3_test_df = complete_df_gb.loc[next3_test_ids['ID']]\n",
                "mimic4_next3_test_df.to_csv(processed_balanced + '/data_mimic4_next3_test.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_few_shot_datasets_next3(num, ids, path_save):\n",
                "    few_shot = ids.sample(n=num, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    df_train = pd.DataFrame(few_shot_train, columns =['ID'])\n",
                "    df_valid = pd.DataFrame(few_shot_valid, columns =['ID'])\n",
                "    df_train.to_csv(path_save + '/m4_next3_train_{}.csv'.format(num))\n",
                "    df_valid.to_csv(path_save + '/m4_next3_valid_{}.csv'.format(num))\n",
                "    few_shot_train_df=complete_df_gb.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=complete_df_gb.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save + '/m4_next3_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save + '/m4_next3_{}_valid.csv'.format(num), index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "next3_tv_ids = tvt_ids.drop(next3_test_ids.index)\n",
                "generate_few_shot_datasets_next3(num=100, ids=next3_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_next3(num=250, ids=next3_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_next3(num=500, ids=next3_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_next3(num=1000, ids=next3_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_next3(num=2000, ids=next3_tv_ids, path_save=processed_balanced)\n",
                "generate_few_shot_datasets_next3(num=3000, ids=next3_tv_ids, path_save=processed_balanced)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "leit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
        },
        "vscode": {
            "interpreter": {
                "hash": "5c6db37f2dbfa0dc7724e0c837d07e3540b86643967779554e04bc9c17696e47"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}