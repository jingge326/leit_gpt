{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pathlib\n",
                "import shutil\n",
                "from datetime import timedelta\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "path_mimic4 = pathlib.Path(os.getcwd()).parents[1]/'data/mimic4'\n",
                "path_processed = path_mimic4/'processed'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_2150806/3857814374.py:3: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                        "  inputs_df = pd.read_csv(path_processed/'tables/inputs_processed.csv')[\n",
                        "/tmp/ipykernel_2150806/3857814374.py:7: DtypeWarning: Columns (8,11,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                        "  presc_df = pd.read_csv(path_processed/'tables/prescriptions_processed.csv')[\n"
                    ]
                }
            ],
            "source": [
                "lab_df = pd.read_csv(path_processed/'tables/lab_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'valuenum', 'label']]\n",
                "inputs_df = pd.read_csv(path_processed/'tables/inputs_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'amount', 'label']]\n",
                "outputs_df = pd.read_csv(path_processed/'tables/outputs_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'value', 'label']]\n",
                "presc_df = pd.read_csv(path_processed/'tables/prescriptions_processed.csv')[\n",
                "    ['subject_id', 'hadm_id', 'charttime', 'dose_val_rx', 'drug']]\n",
                "\n",
                "# Change the name of amount. Valuenum for every table\n",
                "inputs_df['valuenum'] = inputs_df['amount']\n",
                "inputs_df = inputs_df.drop(columns=['amount']).copy()\n",
                "\n",
                "outputs_df['valuenum'] = outputs_df['value']\n",
                "outputs_df = outputs_df.drop(columns=['value']).copy()\n",
                "\n",
                "presc_df['valuenum'] = presc_df['dose_val_rx']\n",
                "presc_df = presc_df.drop(columns=['dose_val_rx']).copy()\n",
                "presc_df['label'] = presc_df['drug']\n",
                "presc_df = presc_df.drop(columns=['drug']).copy()\n",
                "\n",
                "# Tag to distinguish between lab and inputs events\n",
                "inputs_df['Origin'] = 'Inputs'\n",
                "lab_df['Origin'] = 'Lab'\n",
                "outputs_df['Origin'] = 'Outputs'\n",
                "presc_df['Origin'] = 'Prescriptions'\n",
                "\n",
                "merged_df = pd.concat([inputs_df, lab_df, outputs_df, presc_df]).reset_index()\n",
                "\n",
                "# Check that all labels have different names.\n",
                "assert(merged_df['label'].nunique() == (inputs_df['label'].nunique(\n",
                ")+lab_df['label'].nunique()+outputs_df['label'].nunique()+presc_df['label'].nunique()))\n",
                "\n",
                "# set the timestamp as the time delta between the first chart time for each admission\n",
                "merged_df['charttime'] = pd.to_datetime(\n",
                "    merged_df['charttime'], format='%Y-%m-%d %H:%M:%S')\n",
                "ref_time = merged_df.groupby('hadm_id')['charttime'].min()\n",
                "merged_df_1 = pd.merge(ref_time.to_frame(name='ref_time'),\n",
                "                       merged_df, left_index=True, right_on='hadm_id')\n",
                "merged_df_1['time_stamp'] = merged_df_1['charttime']-merged_df_1['ref_time']\n",
                "assert(len(merged_df_1.loc[merged_df_1['time_stamp']\n",
                "       < timedelta(hours=0)].index) == 0)\n",
                "\n",
                "# Create a label code (int) for the labels.\n",
                "label_dict = dict(zip(list(merged_df_1['label'].unique()), range(\n",
                "    len(list(merged_df_1['label'].unique())))))\n",
                "merged_df_1['label_code'] = merged_df_1['label'].map(label_dict)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_df_short = merged_df_1[['hadm_id', 'valuenum', 'time_stamp', 'label_code', 'Origin']].rename(\n",
                "    columns={'hadm_id': 'ID', 'time_stamp': 'Time'})\n",
                "\n",
                "# Make sure that the selected admissions have observations after 24 hours\n",
                "ids_selected = merged_df_short[(merged_df_short['Time'] > timedelta(hours=24)) & \n",
                "                               (merged_df_short['Time'] < timedelta(hours=48))]['ID'].unique()\n",
                "\n",
                "# select only values within first 48 hours\n",
                "merged_df_short = merged_df_short[merged_df_short['ID'].isin(ids_selected)].loc[(merged_df_short['Time'] < timedelta(hours=48))]\n",
                "\n",
                "merged_df_short['Time'] = merged_df_short['Time'].dt.total_seconds().div(60).astype(int)\n",
                "assert(len(merged_df_short.loc[merged_df_short['Time'] > 2880].index) == 0)\n",
                "\n",
                "# drop columns that are not needed for final dataset\n",
                "merged_df_short.drop(['Origin'], axis=1, inplace=True)\n",
                "complete_df = merged_df_short\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name] = 0.0\n",
                        "/tmp/ipykernel_2150806/2855198727.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  complete_df[name2] = 0\n"
                    ]
                }
            ],
            "source": [
                "# create value- and mask- columns and fill with data\n",
                "labels = complete_df['label_code'].unique()\n",
                "value_columns = []\n",
                "mask_columns = []\n",
                "for num in labels:\n",
                "    name = 'Value_label_' + str(num)\n",
                "    name2 = 'Mask_label_' + str(num)\n",
                "    value_columns.append(name)\n",
                "    mask_columns.append(name2)\n",
                "    complete_df[name] = 0.0\n",
                "    complete_df[name2] = 0\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_2150806/939302585.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  mimic4_df = complete_df.groupby(['ID', 'Time'], as_index=False).max()\n",
                        "/tmp/ipykernel_2150806/939302585.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
                        "  mimic4_df = complete_df.groupby(['ID', 'Time'], as_index=False).max()\n"
                    ]
                }
            ],
            "source": [
                "for index, row in complete_df.iterrows():\n",
                "    name = 'Value_label_' + str(row['label_code'].astype(int))\n",
                "    name2 = 'Mask_label_' + str(row['label_code'].astype(int))\n",
                "    complete_df.at[index, name] = row['valuenum']\n",
                "    complete_df.at[index, name2] = 1\n",
                "\n",
                "# drop all unneccesary columns and do sanity check\n",
                "complete_df.drop(['valuenum', 'label_code'], axis=1, inplace=True)\n",
                "\n",
                "# If there are multiple values for the same time stamp, take the maximum\n",
                "mimic4_df = complete_df.groupby(['ID', 'Time'], as_index=False).max()\n",
                "\n",
                "for x in mask_columns:\n",
                "    assert(len(mimic4_df.loc[mimic4_df[x] > 1]) == 0)\n",
                "mimic4_df['ID'] = mimic4_df['ID'].astype(int)\n",
                "\n",
                "mimic4_df.set_index(['ID'], inplace=True)\n",
                "mimic4_df.dropna(inplace=False)\n",
                "mimic4_df.to_csv(path_processed/'mimic4_full_dataset_next.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_state = 1\n",
                "num_test_samples = 2000\n",
                "num_large_m4 = 20000\n",
                "\n",
                "path_r = path_processed/(\"r\"+str(random_state))\n",
                "if not os.path.exists(path_r):\n",
                "    os.mkdir(path_r)\n",
                "\n",
                "mimic4_df = pd.read_csv(path_processed/'mimic4_full_dataset_next.csv', index_col='ID')\n",
                "\n",
                "# Create Next tvt datasets\n",
                "tvt_ids = pd.DataFrame(mimic4_df.index.unique(), columns=['ID'])\n",
                "\n",
                "# Next test dataset \n",
                "next_test_ids = tvt_ids.sample(n=num_test_samples, random_state=random_state)\n",
                "mimic4_next_test_df = mimic4_df.loc[next_test_ids['ID']]\n",
                "mimic4_next_test_df.to_csv(path_r/'data_mimic4_next_test.csv')\n",
                "\n",
                "# Normal next train & validation dataset \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index).sample(n=num_large_m4, random_state=random_state)\n",
                "next_train_ids, next_valid_ids = train_test_split(next_tv_ids, test_size=0.2, random_state=random_state)\n",
                "mimic4_next_train_df = mimic4_df.loc[next_train_ids['ID']]\n",
                "mimic4_next_valid_df = mimic4_df.loc[next_valid_ids['ID']]\n",
                "mimic4_next_train_df.to_csv(path_r/'m4_next_train.csv')\n",
                "mimic4_next_valid_df.to_csv(path_r/'m4_next_valid.csv')\n",
                "\n",
                "def generate_few_shot_datasets_next(num, ids, path_save):\n",
                "    few_shot = ids.sample(n=num, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    few_shot_train_df=mimic4_df.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=mimic4_df.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save/'m4_next_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save/'m4_next_{}_valid.csv'.format(num), index=False)\n",
                "    \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index)\n",
                "generate_few_shot_datasets_next(num=100, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=250, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=500, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=1000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=2000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=3000, ids=next_tv_ids, path_save=path_r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_state = 2\n",
                "num_test_samples = 2000\n",
                "num_large_m4 = 20000\n",
                "\n",
                "path_r = path_processed/(\"r\"+str(random_state))\n",
                "if not os.path.exists(path_r):\n",
                "    os.mkdir(path_r)\n",
                "\n",
                "mimic4_df = pd.read_csv(path_processed/'mimic4_full_dataset_next.csv', index_col='ID')\n",
                "\n",
                "# Create Next tvt datasets\n",
                "tvt_ids = pd.DataFrame(mimic4_df.index.unique(), columns=['ID'])\n",
                "\n",
                "# Next test dataset \n",
                "next_test_ids = tvt_ids.sample(n=num_test_samples, random_state=random_state)\n",
                "mimic4_next_test_df = mimic4_df.loc[next_test_ids['ID']]\n",
                "mimic4_next_test_df.to_csv(path_r/'data_mimic4_next_test.csv')\n",
                "\n",
                "# Normal next train & validation dataset \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index).sample(n=num_large_m4, random_state=random_state)\n",
                "next_train_ids, next_valid_ids = train_test_split(next_tv_ids, test_size=0.2, random_state=random_state)\n",
                "mimic4_next_train_df = mimic4_df.loc[next_train_ids['ID']]\n",
                "mimic4_next_valid_df = mimic4_df.loc[next_valid_ids['ID']]\n",
                "mimic4_next_train_df.to_csv(path_r/'m4_next_train.csv')\n",
                "mimic4_next_valid_df.to_csv(path_r/'m4_next_valid.csv')\n",
                "\n",
                "def generate_few_shot_datasets_next(num, ids, path_save):\n",
                "    few_shot = ids.sample(n=num, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    few_shot_train_df=mimic4_df.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=mimic4_df.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save/'m4_next_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save/'m4_next_{}_valid.csv'.format(num), index=False)\n",
                "    \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index)\n",
                "generate_few_shot_datasets_next(num=100, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=250, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=500, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=1000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=2000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=3000, ids=next_tv_ids, path_save=path_r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_state = 3\n",
                "num_test_samples = 2000\n",
                "num_large_m4 = 20000\n",
                "\n",
                "path_r = path_processed/(\"r\"+str(random_state))\n",
                "if not os.path.exists(path_r):\n",
                "    os.mkdir(path_r)\n",
                "\n",
                "mimic4_df = pd.read_csv(path_processed/'mimic4_full_dataset_next.csv', index_col='ID')\n",
                "\n",
                "# Create Next tvt datasets\n",
                "tvt_ids = pd.DataFrame(mimic4_df.index.unique(), columns=['ID'])\n",
                "\n",
                "# Next test dataset \n",
                "next_test_ids = tvt_ids.sample(n=num_test_samples, random_state=random_state)\n",
                "mimic4_next_test_df = mimic4_df.loc[next_test_ids['ID']]\n",
                "mimic4_next_test_df.to_csv(path_r/'data_mimic4_next_test.csv')\n",
                "\n",
                "# Normal next train & validation dataset \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index).sample(n=num_large_m4, random_state=random_state)\n",
                "next_train_ids, next_valid_ids = train_test_split(next_tv_ids, test_size=0.2, random_state=random_state)\n",
                "mimic4_next_train_df = mimic4_df.loc[next_train_ids['ID']]\n",
                "mimic4_next_valid_df = mimic4_df.loc[next_valid_ids['ID']]\n",
                "mimic4_next_train_df.to_csv(path_r/'m4_next_train.csv')\n",
                "mimic4_next_valid_df.to_csv(path_r/'m4_next_valid.csv')\n",
                "\n",
                "def generate_few_shot_datasets_next(num, ids, path_save):\n",
                "    few_shot = ids.sample(n=num, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    few_shot_train_df=mimic4_df.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=mimic4_df.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save/'m4_next_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save/'m4_next_{}_valid.csv'.format(num), index=False)\n",
                "    \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index)\n",
                "generate_few_shot_datasets_next(num=100, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=250, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=500, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=1000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=2000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=3000, ids=next_tv_ids, path_save=path_r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_state = 4\n",
                "num_test_samples = 2000\n",
                "num_large_m4 = 20000\n",
                "\n",
                "path_r = path_processed/(\"r\"+str(random_state))\n",
                "if not os.path.exists(path_r):\n",
                "    os.mkdir(path_r)\n",
                "\n",
                "mimic4_df = pd.read_csv(path_processed/'mimic4_full_dataset_next.csv', index_col='ID')\n",
                "\n",
                "# Create Next tvt datasets\n",
                "tvt_ids = pd.DataFrame(mimic4_df.index.unique(), columns=['ID'])\n",
                "\n",
                "# Next test dataset \n",
                "next_test_ids = tvt_ids.sample(n=num_test_samples, random_state=random_state)\n",
                "mimic4_next_test_df = mimic4_df.loc[next_test_ids['ID']]\n",
                "mimic4_next_test_df.to_csv(path_r/'data_mimic4_next_test.csv')\n",
                "\n",
                "# Normal next train & validation dataset \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index).sample(n=num_large_m4, random_state=random_state)\n",
                "next_train_ids, next_valid_ids = train_test_split(next_tv_ids, test_size=0.2, random_state=random_state)\n",
                "mimic4_next_train_df = mimic4_df.loc[next_train_ids['ID']]\n",
                "mimic4_next_valid_df = mimic4_df.loc[next_valid_ids['ID']]\n",
                "mimic4_next_train_df.to_csv(path_r/'m4_next_train.csv')\n",
                "mimic4_next_valid_df.to_csv(path_r/'m4_next_valid.csv')\n",
                "\n",
                "def generate_few_shot_datasets_next(num, ids, path_save):\n",
                "    few_shot = ids.sample(n=num, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    few_shot_train_df=mimic4_df.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=mimic4_df.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save/'m4_next_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save/'m4_next_{}_valid.csv'.format(num), index=False)\n",
                "    \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index)\n",
                "generate_few_shot_datasets_next(num=100, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=250, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=500, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=1000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=2000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=3000, ids=next_tv_ids, path_save=path_r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_state = 5\n",
                "num_test_samples = 2000\n",
                "num_large_m4 = 20000\n",
                "\n",
                "path_r = path_processed/(\"r\"+str(random_state))\n",
                "if not os.path.exists(path_r):\n",
                "    os.mkdir(path_r)\n",
                "\n",
                "mimic4_df = pd.read_csv(path_processed/'mimic4_full_dataset_next.csv', index_col='ID')\n",
                "\n",
                "# Create Next tvt datasets\n",
                "tvt_ids = pd.DataFrame(mimic4_df.index.unique(), columns=['ID'])\n",
                "\n",
                "# Next test dataset \n",
                "next_test_ids = tvt_ids.sample(n=num_test_samples, random_state=random_state)\n",
                "mimic4_next_test_df = mimic4_df.loc[next_test_ids['ID']]\n",
                "mimic4_next_test_df.to_csv(path_r/'data_mimic4_next_test.csv')\n",
                "\n",
                "# Normal next train & validation dataset \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index).sample(n=num_large_m4, random_state=random_state)\n",
                "next_train_ids, next_valid_ids = train_test_split(next_tv_ids, test_size=0.2, random_state=random_state)\n",
                "mimic4_next_train_df = mimic4_df.loc[next_train_ids['ID']]\n",
                "mimic4_next_valid_df = mimic4_df.loc[next_valid_ids['ID']]\n",
                "mimic4_next_train_df.to_csv(path_r/'m4_next_train.csv')\n",
                "mimic4_next_valid_df.to_csv(path_r/'m4_next_valid.csv')\n",
                "\n",
                "def generate_few_shot_datasets_next(num, ids, path_save):\n",
                "    few_shot = ids.sample(n=num, random_state=random_state)['ID'].to_list()\n",
                "    few_shot_train, few_shot_valid = train_test_split(few_shot, test_size=0.2, random_state=random_state)\n",
                "    few_shot_train_df=mimic4_df.loc[few_shot_train].reset_index()\n",
                "    few_shot_valid_df=mimic4_df.loc[few_shot_valid].reset_index()\n",
                "    few_shot_train_df.to_csv(path_save/'m4_next_{}_train.csv'.format(num), index=False)\n",
                "    few_shot_valid_df.to_csv(path_save/'m4_next_{}_valid.csv'.format(num), index=False)\n",
                "    \n",
                "next_tv_ids = tvt_ids.drop(next_test_ids.index)\n",
                "generate_few_shot_datasets_next(num=100, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=250, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=500, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=1000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=2000, ids=next_tv_ids, path_save=path_r)\n",
                "generate_few_shot_datasets_next(num=3000, ids=next_tv_ids, path_save=path_r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "leit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        },
        "vscode": {
            "interpreter": {
                "hash": "5c6db37f2dbfa0dc7724e0c837d07e3540b86643967779554e04bc9c17696e47"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
